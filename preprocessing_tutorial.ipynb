{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of EEG data\n",
    "In this notebook, we will go through the basics of preprocessing EEG data using MNE. We will be using the MNE sample data from an experiment. \n",
    "\n",
    "## Using this notebook\n",
    "The code in this notebook is divided into cells. You can execute the code in each cell by pressing the play button (▶) in the top left, or using the shortcut Ctrl+Enter. The output of the code is shown beneath the cell.\n",
    "\n",
    "The workflow of preprocessing is as follows:\n",
    "1. Load the data\n",
    "2. Exclude bad channels\n",
    "3. Common average reference\n",
    "4. Filtering\n",
    "5. Artefact rejection\n",
    "6. Epoching\n",
    "7. Downsampling\n",
    "\n",
    "After preprocessing, we average over trials to get the evoked response and plot the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before beginning we need to make sure we have the python packages we need to run the analysis!\n",
    "\n",
    "The following code chunk creates a virtual environment by running a bash script and installs the requirements specified in `requirements.txt`. In theory you only need to run this code once - as it saves all the packages in a folder called `env`. However, if you update your requirements file you need to run the code again to include new packages. \n",
    "\n",
    "To be able to run this code you need to press \"Select Kernel\" in the top right corner and install the recommended extensions. Choose a python kernel afterwards by clicking the same icon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash setup_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following code runs a bash script, that lets the jupyter kernel know that you have created a new virtual environement that you would like to be able to use. By adding this code to other notebooks, you won't have to install packages every time you start a new uCloud run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash env_to_ipynb_kernel.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!!** Before you can use the packages installed, you need to change the kernel used to run the code in this notebook. Now lets change the kernel to the one including all our installations. Press where it says \"Python....\" in the top right corner, then \"select another kernel\", then \"Jupyter kernels...\" and then select \"env\". If \"env\" does not show up, press the little refresh symbol! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the sample data\n",
    "To begin with we load the MNE sample data. It contains data from an experiment where checkerboard patterns were presented to the subject into the left and right visual (hemi)field, interspersed by tones to the left or right ear. The interval between the stimuli was 750 ms. Occasionally a smiley face was presented at the center of the visual field. The subject was asked to press a key with the right index finger as soon as possible after the appearance of the face.\n",
    "\n",
    "Looking at trials across modalities (auditory/visual) we can see the contrast between auditory and visual processing, while inspecting the left/right trials allows us to observe the contralateral visual processing of the brain (i.e. what is presented to the right visual hemifield is processed in the left visual cortex and vice versa).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file & loading in the data\n",
    "sample_data_folder = mne.datasets.sample.data_path()\n",
    "sample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n",
    "                                    'sample_audvis_raw.fif')\n",
    "raw = mne.io.read_raw_fif(sample_data_raw_file) # raw is an MNE object that contains the data of the class Raw\n",
    "raw.info['bads'] = []\n",
    "\n",
    "raw.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "Looking at the ouput from the load_data() function, answer the following questions:\n",
    "\n",
    "**Q1:** How many EEG channels are there?\n",
    "**A:** \n",
    "\n",
    "**Q2:** Were any EEG channels marked as bad during recording?\n",
    "**A:**\n",
    "\n",
    "**Q3:** What is the sampling frequency?\n",
    "**A:**\n",
    "\n",
    "**Q4:** How many minutes of data were recorded?\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets try and plot the raw data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only EEG and stimulus data & plotting raw data\n",
    "raw.pick(['eeg', 'stim'])\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exclude bad channels\n",
    "When plotting all channels simultaneously, it is evident that one of the channels is not really picking up any signal (that is, it is flat). This is a bad channel and should be excluded from further analysis. There are other ways that channels might be bad, such as being too noisy. In this case, we will just exclude the channel that is flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking the channel as bad\n",
    "raw.info['bads'] = ['EEG 053']\n",
    "\n",
    "# Checking that we have marked the correct channel as bad\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the bad channel\n",
    "raw.pick(['eeg', 'stim'], exclude='bads')\n",
    "\n",
    "# Plotting the data again to see that the bad channel has been removed\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common average reference\n",
    "The idea behind average reference is to compute the average of the signal at all EEG electrodes and subtract it from the EEG signal at every electrode for every time point. To set this “virtual reference” that is the average of all channels, you can use set_eeg_reference() with ref_channels='average'. This is done after excluding bad channels, so the average is computed only over the good channels.\n",
    "\n",
    "The rationale behind this is that the average of all the potentials recorded on the whole head due to current sources inside it is zero, this would make for a quiet or electrically neutral reference. However, in practice, to achieve such an ideal reference one would require large number of electrodes that cover the whole head uniformly, which is not the case in EEG recordings where limited number of electrodes cover mostly the upper part of the head.\n",
    "\n",
    "**IMPORTANT:** The data used here has already been referenced to the average of all channels, but this code will be needed when you analyse your own data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common average reference\n",
    "# raw.set_eeg_reference('average', projection=True)\n",
    "\n",
    "# applying the reference\n",
    "# raw.apply_proj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering\n",
    "Now let's filter the data. We apply a high-pass filter at 0.1 Hz and a low-pass filter at 40 Hz, following the typical practises of EEG preprocessing. The high-pass filter minimises slow drifts in the data (e.g. scalp potentials), while the low-pass filter excludes high-frequency noise, e.g. line noise (50 Hz or 60 Hz) or EMG (muscle-related artefacts), with frequencies higher than the frequencies of the signal we are interested in.\n",
    "\n",
    "**NOTE** MÅSKE FOR TEKNISK???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-pass filtering the data at 0.1 Hz and subsequently low-pass filtering at 40 Hz\n",
    "raw = raw.filter(0.1, None)\n",
    "raw = raw.filter(None, 40)\n",
    "\n",
    "# plotting the filtered data for inspection\n",
    "raw.plot(n_channels=60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "**Q5:** If you compare the raw data with the filtered data, what differences do you see?\n",
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Artefact rejection\n",
    "Not all noise is outside the frequency range of brain activity and can therefore not just be suppressed by filtering. Some artefacts, like eye blinks, comprise large jumps in activity. Since such large shifts are unlikely to be related to brain activity, we can exclude parts of the recording where this happens. We do this by selecting a threshold value for the peak-to-peak amplitude. When we epoch our data, we exclude epochs that contain \"jumps\" of more than ±150 microvolts in the signal, i.e. epochs where the subject likely blinked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejecting everything over or under a threshold of 150 microvolts\n",
    "# anything above or belove this threshold is likely not brain activity but artefacts\n",
    "\n",
    "# for now we are just defining the threshold, we will apply it later when we are creating epochs\n",
    "reject = dict(eeg=150e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Epoching\n",
    "Now that the data has been preprocessed, we can epoch the data. Epoching is the process of cutting the continuous data into smaller segments, called epochs. Each epoch is a time window of the data, centered around an event of interest.\n",
    "\n",
    "To begin with we find the events that were recorded during the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function locates any stimulus events in the recording (e.g. presentation of stimuli or button presses)\n",
    "events = mne.find_events(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make a dictionary of what the event IDs represent\n",
    "# by using '/' we can actually later index one dimension *across* the other, i.e. if we just write 'left' we get all events presented to the left side, both auditory and visual\n",
    "event_id = {'auditory/left': 1,\n",
    "              'auditory/right': 2,\n",
    "              'visual/left': 3,\n",
    "              'visual/right': 4,\n",
    "              'smiley': 5,\n",
    "              'button': 32\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the events\n",
    "mne.viz.plot_events(events, sfreq=250, first_samp=raw.first_samp, event_id=event_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify the timepoints during the recording where both visual and auditory stimuli were presented, as well as when the subject was presented with a smiley and responded with a button press. Since we are interested in the presentation of stimuli, we create our epochs around these events. The time window we establish span from 0.2 seconds before stimulus onset to 0.5 seconds after stimulus onset. The 200 milliseconds before the onset of the stimulus enables us to establish a baseline of \"no activity\", i.e. without stimulus presentation, which we can subtract from our time window with activity in order subtract away the background activity. The 500 milliseconds after the stimulus onset denote the time in which we expect the effect to occur, since the relevant EEG components for this kind of task arise within 500 milliseconds of stimulus onset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing time window\n",
    "tmin, tmax = -0.2, 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the epochs using the variables created in the cell above, and timelocking to the events\n",
    "# baseline time interval spans from beginning of the data (-0.2 s) to 0 s (stimulus onset)\n",
    "# we use the reject variable we created earlier in order to remove artefacts\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, picks=[\"eeg\"],\n",
    "                    baseline=(None, 0), reject=reject, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are dropping some epochs that were deeemed 'bad' because they contain peak-to-peak values over the threshold we defined, allowing us to exclude epochs with artefacts in order to try to keep as much noise out of our data as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Down sampling\n",
    "To reduce the amount of data we have to work with as well as the amount of time it takes to run the analysis the data is downsampled. This is done after creating the epochs, as doing so before can potentially mess with the precision of the extraction of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling to 250 Hz\n",
    "epochs_resampled = epochs.resample(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis (Event related potentials)\n",
    "We can now average over epochs across our modalities (auditory/visual) and take a look at whether there seems to be a difference between our conditions when we average out the noise from individual trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the '/' used in the event IDs comes in handy! To get all epochs with auditory stimuli, we can index 'auditory' across the left/right dimension (and 'visual', of course)\n",
    "# that way we don't have to specify 'auditory_left' and 'auditory_right' etc.\n",
    "aud_epochs = epochs_resampled['auditory']\n",
    "vis_epochs = epochs_resampled['visual']\n",
    "\n",
    "# creating evokeds for auditory condition by averaging over epochs\n",
    "aud_evoked = aud_epochs.average()\n",
    "\n",
    "# creating evokeds for visual condition\n",
    "vis_evoked = vis_epochs.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# comparing the two conditions\n",
    "mne.viz.plot_compare_evokeds(dict(auditory=aud_evoked, visual=vis_evoked),\n",
    "                             legend='upper left', show_sensors='upper right');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
